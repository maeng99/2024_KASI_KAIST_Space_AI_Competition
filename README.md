<img src="https://github.com/user-attachments/assets/5ab779b6-ab38-474e-92a4-e481ea11f639" width="100%"/>
<br />
<br />

# â˜€ï¸[Total 1st íƒœì–‘ìƒ ìˆ˜ìƒ] 2024 ì œ1íšŒ í•œêµ­ì²œë¬¸ì—°êµ¬ì›-ì¹´ì´ìŠ¤íŠ¸ ì²œë¬¸ìš°ì£¼ AI ê²½ì§„ëŒ€íšŒ (ìš°ì­ˆì­ˆíŒ€)â˜€ï¸
- "2024 ì œ1íšŒ í•œêµ­ì²œë¬¸ì—°êµ¬ì›-ì¹´ì´ìŠ¤íŠ¸ ì²œë¬¸ìš°ì£¼ AI ê²½ì§„ëŒ€íšŒ" íƒœì–‘ìƒ ìˆ˜ìƒ ğŸ†
    - í•œêµ­ì²œë¬¸ì—°êµ¬ì› Â· ì¹´ì´ìŠ¤íŠ¸ SWêµìœ¡ì„¼í„° ì£¼ê´€
    - "ì²œë¬¸ì—°, ì²œë¬¸ìš°ì£¼AI ê²½ì§„ëŒ€íšŒÂ·Â·Â·1ìœ„ ìƒëª…ëŒ€, 2ìœ„ KAIST" (https://www.hellodd.com/news/articleView.html?idxno=105351)
<div align="center">
    <img src="https://github.com/user-attachments/assets/65a385d0-9506-4600-8bb8-4888585ee6a0" width="250px"/>
    <img src="https://github.com/user-attachments/assets/d1f2755a-4308-4f83-9d80-d93f19f179e4" width="500px"/>
</div>
<br/>

## 1. Contest Overview
- **ëŒ€íšŒ ì£¼ì œ: AIê¸°ë°˜ ì§€êµ¬ ì˜í–¥ íƒœì–‘ ì´ë²¤íŠ¸ ìë™í™” íƒì§€**
    * íƒœì–‘ ì½”ë¡œë‚˜í™€(coronal hole), í‘ì (sunspot), í™ì—¼(prominence) íƒì§€
<div align="center">
    <img src="https://github.com/user-attachments/assets/b84f0583-4dd0-41fe-9cfe-2683a62a7a23" width="250px"/>
    <img src="https://github.com/user-attachments/assets/13f6be21-0fc6-457d-b399-cbdb8ebe7e5b" width="250px"/>
    <img src="https://github.com/user-attachments/assets/76dccb55-1627-49ee-91ff-55b4101590d4" width="250px"/>
</div>
<br/>

## 2. Team Members
**Team: ìš°ì­ˆì­ˆíŒ€**
| ë§¹ì˜í˜„ | ì‹ ì€ë¹ˆ | ì´ì°½ë¯¼ |
|:------:|:------:|:------:|
| [GitHub](https://github.com/maeng99) | [GitHub](https://github.com/) | [GitHub](https://github.com/) |
<br/>

## 3. Preliminary
- ì²œë¬¸ Â· AI í€´ì¦ˆ + ì²œë¬¸ ë°ì´í„° ë ˆì´ë¸”ë§ ìŠ¤ì½”ì–´
    - ë ˆì´ë¸”ë§ ì˜ˆì‹œ
<div align="center">
    <img src="https://github.com/user-attachments/assets/ff510567-573d-4f64-a88b-8f09b31a3775" width="250px"/>
    <img src="https://github.com/user-attachments/assets/ce63619f-2266-42ce-aeea-1fccb010959c" width="250px"/>
    <img src="https://github.com/user-attachments/assets/f918a4b2-3603-4646-b47c-05a9337de79c" width="250px"/>
</div>
<br />

## 4. Finals
- ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ íƒœì–‘ ì´ë¯¸ì§€ì˜ ë ˆì´ë¸”ì„ íŒë³„í•˜ê³  ìœ„ì¹˜ë¥¼ íƒì§€
    - í‘ì , ì½”ë¡œë‚˜ í™€, í™ì—¼ 3ê°€ì§€ ìœ í˜•ì˜ ì´ë¯¸ì§€ ì œê³µ
    - ê° ì´ë¯¸ì§€ì—ëŠ” ê° íƒì§€ ëŒ€ìƒì— ëŒ€í•œ ë¼ë²¨ë§ë§Œ ëœ ìƒíƒœ
- elice í”Œë«í¼( https://kaist-kasiai.elice.io/explore )ì„ í™œìš©í•´ ê³¼ì œ ìˆ˜í–‰
### 4.1 Evaluation Formula
- F1 Score
    - IoU 0.5
<br />

## 5. Key Points
### 5.1 Pipeline Construction
### 5.2 Augmentation
### 5.3 Epoch
<br />


## 6. Final Code
### 6.1 Classify Datasets by Solar Event
#### 6.1.1 Library Declaration
```python
import json
import os
import shutil

import matplotlib.patches as patches
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yaml
from PIL import Image
from tqdm import tqdm

from sklearn.model_selection import train_test_split
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import torch.nn as nn
import torch.optim as optim
```
#### 6.1.2 Define HyperParameters
```python
IMAGE_SIZE = 1024
BATCH = 64
EPOCH = 70
```
#### 6.1.3 Load Training Data
```python
train_data = []

for image in tqdm(os.listdir(os.path.join(DATASET_ROOT, TRAIN_DIR, IMAGE_DIR))):
    image_id = image.split(".")[0]
    image_path = os.path.join(DATASET_ROOT, TRAIN_DIR, IMAGE_DIR, image)
    label_path = os.path.join(DATASET_ROOT, TRAIN_DIR, LABELS_DIR, image_id + ".txt")
    labels = []
    if os.path.exists(label_path):
        with open(label_path, "r") as f:
            for line in f.readlines():
                class_id = int(line.split()[0])
                x = float(line.split()[1])
                y = float(line.split()[2])
                w = float(line.split()[3])
                h = float(line.split()[4])
                labels.append({"class_id": class_id, "x": x, "y": y, "w": w, "h": h})

    train_data.append({"id": image_id, "image_path": image_path, "label_path": label_path, "labels": labels})

df_train = pd.DataFrame(train_data)
df_train.head()
```
#### 6.1.4 Augmentation
```python
# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì— ì¦ê°• ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ë„ë¡ ì„¤ì •
augmented_image_dir = os.path.join(os.getcwd(), "augmented_images")
augmented_label_dir = os.path.join(os.getcwd(), "augmented_labels")
os.makedirs(augmented_image_dir, exist_ok=True)
os.makedirs(augmented_label_dir, exist_ok=True)

# ì´ë¯¸ì§€ë¥¼ ë°˜ì „í•˜ê³  ë¼ë²¨ì„ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def flip_image_and_labels(image, labels, flip_type):
    if flip_type == 'tb':  # ìƒí•˜ ë°˜ì „
        flipped_image = image.transpose(method=Image.FLIP_TOP_BOTTOM)
        flipped_labels = [{'class_id': label['class_id'], 'x': label['x'], 'y': 1 - label['y'], 'w': label['w'], 'h': label['h']} for label in labels]
    elif flip_type == 'lr':  # ì¢Œìš° ë°˜ì „
        flipped_image = image.transpose(method=Image.FLIP_LEFT_RIGHT)
        flipped_labels = [{'class_id': label['class_id'], 'x': 1 - label['x'], 'y': label['y'], 'w': label['w'], 'h': label['h']} for label in labels]
    elif flip_type == 'tb_lr':  # ìƒí•˜ ë° ì¢Œìš° ë°˜ì „
        flipped_image = image.transpose(method=Image.FLIP_TOP_BOTTOM).transpose(method=Image.FLIP_LEFT_RIGHT)
        flipped_labels = [{'class_id': label['class_id'], 'x': 1 - label['x'], 'y': 1 - label['y'], 'w': label['w'], 'h': label['h']} for label in labels]
    else:
        flipped_image = image
        flipped_labels = labels
    
    return flipped_image, flipped_labels

# ë°˜ì „ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_augmented_data(image_path, labels, output_image_path, output_label_path, flip_type):
    # ì´ë¯¸ì§€ ì—´ê¸°
    image = Image.open(image_path)
    
    # ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë°˜ì „
    flipped_image, flipped_labels = flip_image_and_labels(image, labels, flip_type)

    # ë°˜ì „ëœ ì´ë¯¸ì§€ ì €ì¥
    flipped_image.save(output_image_path)

    # ë°˜ì „ëœ ë¼ë²¨ ì €ì¥
    with open(output_label_path, 'w') as f:
        for label in flipped_labels:
            f.write(f"{label['class_id']} {label['x']} {label['y']} {label['w']} {label['h']}\n")

# ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° ì¦ê°•
for i, row in tqdm(df_train.iterrows(), total=len(df_train)):
    image_id = row['id']
    image_path = row['image_path']
    label_path = row['label_path']

    # ì›ë³¸ ë¼ë²¨ ë¶ˆëŸ¬ì˜¤ê¸°
    labels = []
    with open(label_path, 'r') as f:
        for line in f.readlines():
            class_id, x, y, w, h = map(float, line.strip().split())
            labels.append({'class_id': class_id, 'x': x, 'y': y, 'w': w, 'h': h})

    # ì¦ê°•ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ê° ë°˜ì „ íƒ€ì…ë³„ íŒŒì¼ëª…ê³¼ ê²½ë¡œ
    flip_types = ['', 'tb', 'lr']
    for flip_type in flip_types:
        new_image_id = f"{image_id}_aug_{flip_type}" if flip_type else f"{image_id}_aug"
        output_image_path = os.path.join(augmented_image_dir, f"{new_image_id}.jpg")
        output_label_path = os.path.join(augmented_label_dir, f"{new_image_id}.txt")

        # ì¦ê°• ë°ì´í„° ì €ì¥
        save_augmented_data(image_path, labels, output_image_path, output_label_path, flip_type)

        # ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ í–‰ ì¶”ê°€
        new_row = pd.DataFrame([{
            'id': new_image_id,
            'image_path': output_image_path,
            'label_path': output_label_path,
            'labels': labels
        }])

        # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì— ì¦ê°•ëœ ë°ì´í„° ì¶”ê°€
        df_train = pd.concat([df_train, new_row], ignore_index=True)
```
#### 6.1.5 Define "class" Column
- "labels' columnì˜ 'class_id'ë¥¼ ì´ìš©í•˜ì—¬ "class" column ì •ì˜
```python
df_train['class'] = df_train['labels'].apply(lambda x: [item['class_id'] for item in x] if x else [])
df_train['class'] = df_train['class'].apply(lambda x: int(x[0]) if x else None)
df_train['class'] = df_train['class'].apply(lambda x: int(x) if pd.notna(x) else -1)
df_train['class'].value_counts()
```
```
class
 0    20504
 2    16876
 1    16444
-1     5644
Name: count, dtype: int6
```
#### 6.1.6 Create "df_train_NaN" and "df_train_noneNaN" DataFrame
- labelì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ì˜ ì •ë³´ë¥¼ ëª¨ì•„ "df_train_NaN" ìƒì„±
- labelì´ ì¡´ì¬í•˜ëŠ” ì´ë¯¸ì§€ì˜ ì •ë³´ë¥¼ ëª¨ì•„ "df_train_noneNaN" ìƒì„±
```python
df_train_noneNaN = df_train[df_train['class'] != -1]
df_train_NaN = df_train[df_train['class'] == -1]
```
#### 6.1.7 Define and Train CNN Model (Classification)
```python
image_size = (256, 256)
batch_size = 32
num_classes = df_train['class'].nunique()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •
transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.ToTensor(),
])
```
```python
# CNN ëª¨ë¸ ì •ì˜
class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * (image_size[0]//8) * (image_size[1]//8), 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(-1, 128 * (image_size[0]//8) * (image_size[1]//8))
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```
```python
# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class ImageDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['image_path']
        image = Image.open(img_path).convert('RGB')
        label = int(self.df.iloc[idx]['class'])

        if self.transform:
            image = self.transform(image)

        return image, label
```
```python
# ë°ì´í„°ì…‹ ìƒì„±
train_df, valid_df = train_test_split(df_train_noneNaN, test_size=0.2, random_state=42)
train_dataset = ImageDataset(train_df, transform=transform)
valid_dataset = ImageDataset(valid_df, transform=transform)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = SimpleCNN(num_classes=num_classes)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# í•™ìŠµ í•¨ìˆ˜ ì •ì˜
def train_model(model, train_loader, criterion, optimizer, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

# ê²€ì¦ í•¨ìˆ˜ ì •ì˜
def validate_model(model, valid_loader, criterion):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in valid_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Validation Accuracy: {100 * correct / total:.2f}%')
    
# ëª¨ë¸ í•™ìŠµ
train_model(model, train_loader, criterion, optimizer, num_epochs=4)
# ëª¨ë¸ ê²€ì¦
validate_model(model, valid_loader, criterion)
# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), 'simple_cnn.pth')
```
#### 6.1.8 Classify "df_train_NaN" datasets
- classê°€ ì—†ëŠ” ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ CNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜
```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class NaNDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['image_path']
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ìƒì„±
NaN_dataset = NaNDataset(df_train_NaN, transform=transform)
NaN_loader = DataLoader(NaN_dataset, batch_size=batch_size, shuffle=False)

# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
predictions_NaN = []

with torch.no_grad():
    for images in NaN_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        predictions_NaN.extend(predicted.cpu().numpy())

# df_testì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
df_train_NaN['class'] = predictions_NaN
```
#### 6.1.9 Concat "df_train_NaN" and "df_train_noneNaN" DataFrame
```python
df_train = pd.concat([df_train_noneNaN,df_train_NaN]).sort_index()
```
---
### 6.2 Training Models for Each Solar Event
#### 6.2.1 Coronal Hole: Split Data
- train data:valid data = 8:2
```python
df_train_coronalHole_set = df_train_coronalHole.sample(frac=0.8, random_state=0)
df_valid_coronalHole_set = df_train_coronalHole.drop(df_train_coronalHole_set.index)
```
#### 6.2.2 Coronal Hole: Data Preprocessing
- resize the image and copy it to each folder
```python
for i, row in tqdm(df_train_coronalHole_set.iterrows(), total=len(df_train_coronalHole_set)):
    image = Image.open(row["image_path"])
    image.resize((IMAGE_SIZE, IMAGE_SIZE)).save(f"{new_train_coronalHole_path}/{IMAGE_DIR}/{row['id']}.jpg")
    shutil.copy(row["label_path"], f"{new_train_coronalHole_path}/{LABELS_DIR}/{row['id']}.txt")

for i, row in tqdm(df_valid_coronalHole_set.iterrows(), total=len(df_valid_coronalHole_set)):
    image = Image.open(row["image_path"])
    image.resize((IMAGE_SIZE, IMAGE_SIZE)).save(f"{new_valid_coronalHole_path}/{IMAGE_DIR}/{row['id']}.jpg")
    shutil.copy(row["label_path"], f"{new_valid_coronalHole_path}/{LABELS_DIR}/{row['id']}.txt")
```
