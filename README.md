<img src="https://github.com/user-attachments/assets/5ab779b6-ab38-474e-92a4-e481ea11f639" width="100%"/>
<br />
<br />

# â˜€ï¸[Total 1st íƒœì–‘ìƒ ìˆ˜ìƒ] 2024 ì œ1íšŒ í•œêµ­ì²œë¬¸ì—°êµ¬ì›-ì¹´ì´ìŠ¤íŠ¸ ì²œë¬¸ìš°ì£¼ AI ê²½ì§„ëŒ€íšŒ (ìš°ì­ˆì­ˆíŒ€)â˜€ï¸
- "2024 ì œ1íšŒ í•œêµ­ì²œë¬¸ì—°êµ¬ì›-ì¹´ì´ìŠ¤íŠ¸ ì²œë¬¸ìš°ì£¼ AI ê²½ì§„ëŒ€íšŒ" íƒœì–‘ìƒ ìˆ˜ìƒ ğŸ†
    - í•œêµ­ì²œë¬¸ì—°êµ¬ì› Â· ì¹´ì´ìŠ¤íŠ¸ SWêµìœ¡ì„¼í„° ì£¼ê´€
    - "ì²œë¬¸ì—°, ì²œë¬¸ìš°ì£¼AI ê²½ì§„ëŒ€íšŒÂ·Â·Â·1ìœ„ ìƒëª…ëŒ€, 2ìœ„ KAIST" (https://www.hellodd.com/news/articleView.html?idxno=105351)
<div align="center">
    <img src="https://github.com/user-attachments/assets/65a385d0-9506-4600-8bb8-4888585ee6a0" width="250px"/>
    <img src="https://github.com/user-attachments/assets/d1f2755a-4308-4f83-9d80-d93f19f179e4" width="500px"/>
</div>
<br/>

## 1. Contest Overview
- ëŒ€íšŒ ì£¼ì œ: **"AIê¸°ë°˜ ì§€êµ¬ ì˜í–¥ íƒœì–‘ ì´ë²¤íŠ¸ ìë™í™” íƒì§€"**
    * íƒœì–‘ ì½”ë¡œë‚˜í™€(coronal hole), í‘ì (sunspot), í™ì—¼(prominence) íƒì§€
<div align="center">
    <img src="https://github.com/user-attachments/assets/b84f0583-4dd0-41fe-9cfe-2683a62a7a23" width="250px"/>
    <img src="https://github.com/user-attachments/assets/13f6be21-0fc6-457d-b399-cbdb8ebe7e5b" width="250px"/>
    <img src="https://github.com/user-attachments/assets/76dccb55-1627-49ee-91ff-55b4101590d4" width="250px"/>
</div>

- ëŒ€íšŒ ê¸°ê°„: 2024.07. ~ 2024.08. (1ê°œì›”)
<br/>

## 2. Team Members
Team: **"ìš°ì­ˆì­ˆíŒ€"**
| ë§¹ì˜í˜„ | ì‹ ì€ë¹ˆ | ì´ì°½ë¯¼ |
|:------:|:------:|:------:|
| [GitHub](https://github.com/maeng99) | [GitHub](https://github.com/) | [GitHub](https://github.com/LeeChangmin0310) |
<br/>

## 3. Preliminary
- ì²œë¬¸ Â· AI í€´ì¦ˆ + ì²œë¬¸ ë°ì´í„° ë ˆì´ë¸”ë§ ìŠ¤ì½”ì–´
    - ë ˆì´ë¸”ë§ ì˜ˆì‹œ
<div align="center">
    <img src="https://github.com/user-attachments/assets/ff510567-573d-4f64-a88b-8f09b31a3775" width="250px"/>
    <img src="https://github.com/user-attachments/assets/ce63619f-2266-42ce-aeea-1fccb010959c" width="250px"/>
    <img src="https://github.com/user-attachments/assets/f918a4b2-3603-4646-b47c-05a9337de79c" width="250px"/>
</div>
<br />

## 4. Finals
- ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ íƒœì–‘ ì´ë¯¸ì§€ì˜ ë ˆì´ë¸”ì„ íŒë³„í•˜ê³  ìœ„ì¹˜ë¥¼ íƒì§€
    - í‘ì , ì½”ë¡œë‚˜ í™€, í™ì—¼ 3ê°€ì§€ ìœ í˜•ì˜ ì´ë¯¸ì§€ ì œê³µ
    - ê° ì´ë¯¸ì§€ì—ëŠ” ê° íƒì§€ ëŒ€ìƒì— ëŒ€í•œ ë¼ë²¨ë§ë§Œ ëœ ìƒíƒœ
- elice í”Œë«í¼( https://kaist-kasiai.elice.io/explore )ì„ í™œìš©í•´ ê³¼ì œ ìˆ˜í–‰
### 4.1 Evaluation Formula
- F1 Score
    - IoU 0.5
- Public í‰ê°€ ë°ì´í„°ì™€ Private í‰ê°€ ë°ì´í„° 50:50ìœ¼ë¡œ ìµœì¢… ì±„ì 
### 4.2 Fianl Results of The Evaluation
- Public: 0.884474 / Private: 0.884492 / Total: 0.884483
<img src="https://github.com/user-attachments/assets/5d4c00a3-3823-4fd2-9950-22746d0f3990" width="800px"/>
<br />
<br />

## 5. Key Points
### 5.1 Pipeline Construction

#### [ Overview ]
![Pipeline Overview](pipeline_images/Pipeline_overview.png)

##### [ Stage 1 ]
![Pipeline Stage1](pipeline_images/Stage_1.png)

#### [ Stage 2 ]
![Pipeline Stage2](pipeline_images/Stage_2.png)

- ëŒ€íšŒë¥¼ ì„±ê³µì ìœ¼ë¡œ ì´ëŒì—ˆë˜ ì£¼ìš”ì¸
### 5.2 YOLO Models
- detectionì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì—ì„œ YOLOv5në¶€í„° YOLOv10sì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ modelë¡œ ê³¼ì œë¥¼ ìˆ˜í–‰
- ì˜ì™¸ë¡œ YOLOv5s modelì´ ê°€ì¥ ë†’ì€ ë‹¬ì„±ë¥ ì„ ë³´ì„<br /> -> ì•„ë§ˆ ë°ì´í„°ê°€ ì—„ì²­ ë°©ëŒ€í•˜ì§€ ì•Šê³ , ë³µì¡í•œ í˜•íƒœê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°...
### 5.3 Augmentation
- ì—¬ëŸ¬ ë°©ì‹ì˜ ì¦ê°• ê¸°ë²• ì‹œë„
- ê° íƒœì–‘ ì´ë²¤íŠ¸ ë³„ ì´ë¯¸ì§€ í˜•íƒœê°€ ë¹„ìŠ·í•˜ë‹¤ëŠ” ì ê³¼ íƒœì–‘ ì´ë²¤íŠ¸ê°€ ê·¹ì ì—ì„œ ì£¼ë¡œ ë°œìƒí•œë‹¤ëŠ” ì ì„ ê³ ë ¤í•˜ì—¬,<br />ìƒí•˜ ë˜ëŠ” ì¢Œìš° Flip ê¸°ë²•ì„ ì±„íƒí•˜ì—¬ ë‹¤ì–‘í•˜ê²Œ ì ìš©
### 5.4 Image Size / Batch / Epoch
- ì‹¤í—˜ì€ í†µí•œ ìµœì ì˜ HyperParameter ê²°ì •
- Image Size=1024 / Batch=64 / Epoch=70
<br />
<br />

## 6. Final Code
### 6.1 Classify Datasets by Solar Event
#### 6.1.1 Declare Library
```python
import json
import os
import shutil

import matplotlib.patches as patches
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yaml
from PIL import Image
from tqdm import tqdm

from sklearn.model_selection import train_test_split
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import torch.nn as nn
import torch.optim as optim
```
#### 6.1.2 Define HyperParameters
```python
IMAGE_SIZE = 1024
BATCH = 64
EPOCH = 70
```
#### 6.1.3 Load Train Data
```python
train_data = []

for image in tqdm(os.listdir(os.path.join(DATASET_ROOT, TRAIN_DIR, IMAGE_DIR))):
    image_id = image.split(".")[0]
    image_path = os.path.join(DATASET_ROOT, TRAIN_DIR, IMAGE_DIR, image)
    label_path = os.path.join(DATASET_ROOT, TRAIN_DIR, LABELS_DIR, image_id + ".txt")
    labels = []
    if os.path.exists(label_path):
        with open(label_path, "r") as f:
            for line in f.readlines():
                class_id = int(line.split()[0])
                x = float(line.split()[1])
                y = float(line.split()[2])
                w = float(line.split()[3])
                h = float(line.split()[4])
                labels.append({"class_id": class_id, "x": x, "y": y, "w": w, "h": h})

    train_data.append({"id": image_id, "image_path": image_path, "label_path": label_path, "labels": labels})

df_train = pd.DataFrame(train_data)
df_train.head()
```
#### 6.1.4 Augmentation
```python
# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì— ì¦ê°• ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ë„ë¡ ì„¤ì •
augmented_image_dir = os.path.join(os.getcwd(), "augmented_images")
augmented_label_dir = os.path.join(os.getcwd(), "augmented_labels")
os.makedirs(augmented_image_dir, exist_ok=True)
os.makedirs(augmented_label_dir, exist_ok=True)

# ì´ë¯¸ì§€ë¥¼ ë°˜ì „í•˜ê³  ë¼ë²¨ì„ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def flip_image_and_labels(image, labels, flip_type):
    if flip_type == 'tb':  # ìƒí•˜ ë°˜ì „
        flipped_image = image.transpose(method=Image.FLIP_TOP_BOTTOM)
        flipped_labels = [{'class_id': label['class_id'], 'x': label['x'], 'y': 1 - label['y'], 'w': label['w'], 'h': label['h']} for label in labels]
    elif flip_type == 'lr':  # ì¢Œìš° ë°˜ì „
        flipped_image = image.transpose(method=Image.FLIP_LEFT_RIGHT)
        flipped_labels = [{'class_id': label['class_id'], 'x': 1 - label['x'], 'y': label['y'], 'w': label['w'], 'h': label['h']} for label in labels]
    elif flip_type == 'tb_lr':  # ìƒí•˜ ë° ì¢Œìš° ë°˜ì „
        flipped_image = image.transpose(method=Image.FLIP_TOP_BOTTOM).transpose(method=Image.FLIP_LEFT_RIGHT)
        flipped_labels = [{'class_id': label['class_id'], 'x': 1 - label['x'], 'y': 1 - label['y'], 'w': label['w'], 'h': label['h']} for label in labels]
    else:
        flipped_image = image
        flipped_labels = labels
    
    return flipped_image, flipped_labels

# ë°˜ì „ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_augmented_data(image_path, labels, output_image_path, output_label_path, flip_type):
    # ì´ë¯¸ì§€ ì—´ê¸°
    image = Image.open(image_path)
    
    # ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë°˜ì „
    flipped_image, flipped_labels = flip_image_and_labels(image, labels, flip_type)

    # ë°˜ì „ëœ ì´ë¯¸ì§€ ì €ì¥
    flipped_image.save(output_image_path)

    # ë°˜ì „ëœ ë¼ë²¨ ì €ì¥
    with open(output_label_path, 'w') as f:
        for label in flipped_labels:
            f.write(f"{label['class_id']} {label['x']} {label['y']} {label['w']} {label['h']}\n")

# ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° ì¦ê°•
for i, row in tqdm(df_train.iterrows(), total=len(df_train)):
    image_id = row['id']
    image_path = row['image_path']
    label_path = row['label_path']

    # ì›ë³¸ ë¼ë²¨ ë¶ˆëŸ¬ì˜¤ê¸°
    labels = []
    with open(label_path, 'r') as f:
        for line in f.readlines():
            class_id, x, y, w, h = map(float, line.strip().split())
            labels.append({'class_id': class_id, 'x': x, 'y': y, 'w': w, 'h': h})

    # ì¦ê°•ëœ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ê° ë°˜ì „ íƒ€ì…ë³„ íŒŒì¼ëª…ê³¼ ê²½ë¡œ
    flip_types = ['', 'tb', 'lr']
    for flip_type in flip_types:
        new_image_id = f"{image_id}_aug_{flip_type}" if flip_type else f"{image_id}_aug"
        output_image_path = os.path.join(augmented_image_dir, f"{new_image_id}.jpg")
        output_label_path = os.path.join(augmented_label_dir, f"{new_image_id}.txt")

        # ì¦ê°• ë°ì´í„° ì €ì¥
        save_augmented_data(image_path, labels, output_image_path, output_label_path, flip_type)

        # ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ í–‰ ì¶”ê°€
        new_row = pd.DataFrame([{
            'id': new_image_id,
            'image_path': output_image_path,
            'label_path': output_label_path,
            'labels': labels
        }])

        # ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì— ì¦ê°•ëœ ë°ì´í„° ì¶”ê°€
        df_train = pd.concat([df_train, new_row], ignore_index=True)
```
#### 6.1.5 Define "class" Column
- Define "class" column using 'class_id' of "labels" column
```python
df_train['class'] = df_train['labels'].apply(lambda x: [item['class_id'] for item in x] if x else [])
df_train['class'] = df_train['class'].apply(lambda x: int(x[0]) if x else None)
df_train['class'] = df_train['class'].apply(lambda x: int(x) if pd.notna(x) else -1)
df_train['class'].value_counts()
```
```
class
 0    20504
 2    16876
 1    16444
-1     5644
Name: count, dtype: int6
```
#### 6.1.6 Create "df_train_NaN" and "df_train_noneNaN" DataFrame
- Create image information without label as "df_train_NaN" dataFrame
- Create image information with label as "df_train_noneNaN" dataFrame
```python
df_train_noneNaN = df_train[df_train['class'] != -1]
df_train_NaN = df_train[df_train['class'] == -1]
```
#### 6.1.7 Define and Train CNN Model (Classification)
```python
image_size = (256, 256)
batch_size = 32
num_classes = df_train['class'].nunique()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì„¤ì •
transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.ToTensor(),
])
```
```python
# CNN ëª¨ë¸ ì •ì˜
class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * (image_size[0]//8) * (image_size[1]//8), 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(-1, 128 * (image_size[0]//8) * (image_size[1]//8))
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```
```python
# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class ImageDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['image_path']
        image = Image.open(img_path).convert('RGB')
        label = int(self.df.iloc[idx]['class'])

        if self.transform:
            image = self.transform(image)

        return image, label
```
```python
# ë°ì´í„°ì…‹ ìƒì„±
train_df, valid_df = train_test_split(df_train_noneNaN, test_size=0.2, random_state=42)
train_dataset = ImageDataset(train_df, transform=transform)
valid_dataset = ImageDataset(valid_df, transform=transform)

# ë°ì´í„° ë¡œë” ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)

# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = SimpleCNN(num_classes=num_classes)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# í•™ìŠµ í•¨ìˆ˜ ì •ì˜
def train_model(model, train_loader, criterion, optimizer, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')

# ê²€ì¦ í•¨ìˆ˜ ì •ì˜
def validate_model(model, valid_loader, criterion):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in valid_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Validation Accuracy: {100 * correct / total:.2f}%')
    
# ëª¨ë¸ í•™ìŠµ
train_model(model, train_loader, criterion, optimizer, num_epochs=4)
# ëª¨ë¸ ê²€ì¦
validate_model(model, valid_loader, criterion)
# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), 'simple_cnn.pth')
```
#### 6.1.8 Classify "df_train_NaN" Datasets
- Classify image data without class using CNN model
```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class NaNDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['image_path']
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ìƒì„±
NaN_dataset = NaNDataset(df_train_NaN, transform=transform)
NaN_loader = DataLoader(NaN_dataset, batch_size=batch_size, shuffle=False)

# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
predictions_NaN = []

with torch.no_grad():
    for images in NaN_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        predictions_NaN.extend(predicted.cpu().numpy())

# df_testì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
df_train_NaN['class'] = predictions_NaN
```
#### 6.1.9 Concat "df_train_NaN" and "df_train_noneNaN" DataFrame
```python
df_train = pd.concat([df_train_noneNaN,df_train_NaN]).sort_index()
```
---
### 6.2 Training Models for Each Solar Event
#### 6.2.0 Clone YOLOv5 Model
```python
!git clone https://github.com/ultralytics/yolov5.git 
!pip install --quiet -r yolov5/requirements.txt
```
#### 6.2.1 Coronal Hole: Split Data
- train data:valid data = 8:2
```python
df_train_coronalHole_set = df_train_coronalHole.sample(frac=0.8, random_state=0)
df_valid_coronalHole_set = df_train_coronalHole.drop(df_train_coronalHole_set.index)
```
#### 6.2.2 Coronal Hole: Data Preprocessing
- Resize the image and copy it to each folder
```python
for i, row in tqdm(df_train_coronalHole_set.iterrows(), total=len(df_train_coronalHole_set)):
    image = Image.open(row["image_path"])
    image.resize((IMAGE_SIZE, IMAGE_SIZE)).save(f"{new_train_coronalHole_path}/{IMAGE_DIR}/{row['id']}.jpg")
    shutil.copy(row["label_path"], f"{new_train_coronalHole_path}/{LABELS_DIR}/{row['id']}.txt")

for i, row in tqdm(df_valid_coronalHole_set.iterrows(), total=len(df_valid_coronalHole_set)):
    image = Image.open(row["image_path"])
    image.resize((IMAGE_SIZE, IMAGE_SIZE)).save(f"{new_valid_coronalHole_path}/{IMAGE_DIR}/{row['id']}.jpg")
    shutil.copy(row["label_path"], f"{new_valid_coronalHole_path}/{LABELS_DIR}/{row['id']}.txt")
```
#### 6.2.3 Coronal Hole: Train Model
```python
dataset = {
    "path": os.path.abspath("."),
    "train": "train_coronalHole",
    "val": "valid_coronalHole",
    "nc": 1,
    "names": ["coronal_hole"],
}

YAML_PATH = os.path.abspath("coronalHole.yaml")
RESULT_PATH = os.path.abspath("result6_coronalHole")
os.makedirs(RESULT_PATH, exist_ok=True)

with open(YAML_PATH, "w") as f:
    yaml.dump(dataset, f)
```
```python
!PYTHONWARNINGS="ignore::FutureWarning" python yolov5/train.py --img {IMAGE_SIZE} --batch {BATCH} --epoch {EPOCH} --data {YAML_PATH} --cfg yolov5s.yaml --exist-ok --weights "" --name coronalHole --project {RESULT_PATH}
```
```
      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
       0/69      32.8G    0.07525    0.04406          0        217       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.122      0.138      0.048     0.0104

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
       1/69      29.2G    0.05207    0.03072          0        199       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.664      0.707      0.696      0.338

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
       2/69      29.2G    0.04162    0.02532          0        175       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.605      0.519      0.525      0.173

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
       3/69      29.2G    0.03505    0.02265          0        184       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.854      0.852      0.915        0.6

    ...

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
      66/69      32.7G     0.0193    0.01447          0        191       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.963      0.974      0.991      0.829

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
      67/69      32.7G    0.01916    0.01411          0        169       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.963      0.975      0.991       0.83

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
      68/69      32.7G    0.01904    0.01414          0        190       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.963      0.975      0.991      0.831

      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
      69/69      32.7G    0.01891    0.01406          0        223       1024: 1
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.964      0.975      0.991      0.832

YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs
                 Class     Images  Instances          P          R      mAP50   
                   all       5920      12660      0.964      0.975      0.991      0.832
```
#### 6.2.4 Coronal Hole: Validate Model
```python
!python yolov5/detect.py --source {os.path.join(new_valid_coronalHole_path, IMAGE_DIR)} --weights {RESULT_PATH}/coronalHole/weights/best.pt --conf 0.5 --save-txt --save-conf --exist-ok --project {RESULT_PATH}/valid
```
- Visualize predictions (r: predictions, w: labels)
<img src="https://github.com/user-attachments/assets/bb930e43-f898-4483-968b-a52d0dcc59b2" width="400px"/>

#### 6.2.5 Repeat This Training Process Equally for Sunspots and Prominence
...
<br />
<img src="https://github.com/user-attachments/assets/ed9a124a-91d7-453e-ab29-d6c5d8265140" width="400px"/>

---
### 6.3 Inference
#### 6.3.1 Load Test Data
```python
test_data = []

for image in tqdm(os.listdir(os.path.join(DATASET_ROOT, TEST_DIR, IMAGE_DIR))):
    image_id = image.split(".")[0]
    image_path = os.path.join(DATASET_ROOT, TEST_DIR, IMAGE_DIR, image)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.
    test_data.append({"id": image_id, "image_path": image_path, "label_path": "", "labels": []})

df_test = pd.DataFrame(test_data)
```
#### 6.3.2 Classify Test Data
- Classify to use the CNN model trained earlier
```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class TestDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['image_path']
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ìƒì„±
test_dataset = TestDataset(df_test, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
predictions_test = []

with torch.no_grad():
    for images in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        predictions_test.extend(predicted.cpu().numpy())

# df_testì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
df_test['class'] = predictions_test
```
```python
df_test_coronalHole = df_test[df_test['class'] == 0]
df_test_sunspot = df_test[df_test['class'] == 1]
df_test_prominence = df_test[df_test['class'] == 2]
```
#### 6.3.3 Infer for Each Solar Event
- Preprocessing
```python
for i, row in tqdm(df_test_coronalHole.iterrows(), total=len(df_test_coronalHole)):
    image = Image.open(row["image_path"])
    image.resize((IMAGE_SIZE, IMAGE_SIZE)).save(f"{new_test_coronalHole_path}/{IMAGE_DIR}/{row['id']}.jpg")
```
- Test
```python
!python yolov5/detect.py --source {os.path.join(new_test_coronalHole_path, IMAGE_DIR)} --weights result6_coronalHole/coronalHole/weights/best.pt --conf 0.5 --save-txt --save-conf --exist-ok --project result6_coronalHole/test
```
- Create label
```python
for i, row in tqdm(df_test_coronalHole.iterrows(), total=len(df_test_coronalHole)):
    label_path = os.path.join("result6_coronalHole", "test", "exp", "labels", row["id"] + ".txt")

    labels = []
    if os.path.exists(label_path):
        with open(label_path, "r") as f:
            lines = f.readlines()

        for line in lines:
            class_id, c_x, c_y, w, h, conf = map(float, line.split())
            labels.append({"class_id": int(class_id), "conf": conf, "x": c_x, "y": c_y, "w": w, "h": h})

    df_test_coronalHole.at[i, "labels"] = labels
df_test_coronalHole
```
<img src="https://github.com/user-attachments/assets/6ef74853-c749-4707-a4e7-dd1c64527b0b" width="400px"/>

- Repeat This Inference Process Equally for Sunspots and Prominence<br />...
#### 6.3.4 Combine Test Data
```python
df_test = pd.concat([df_test_coronalHole,df_test_sunspot,df_test_prominence]).sort_index()
df_test = df_test.drop('class', axis=1)
df_test.head()
```
#### 6.3.5 Create Submission File
```python
submission = []

for i, row in tqdm(df_test.iterrows(), total=len(df_test)):
    image_id = row["id"]
    labels = []
    for label in row["labels"]:
        class_id = label["class_id"]
        x = label["x"]
        y = label["y"]
        w = label["w"]
        h = label["h"]

        labels.append({"class_id": class_id, "x": x, "y": y, "w": w, "h": h})
    submission.append({"id": image_id, "labels": labels})

df_submission = pd.DataFrame(submission)
df_submission.to_csv("submission.csv", index=False)
```
